{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from torch import Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n",
    "        super(Inception, self).__init__()\n",
    "\n",
    "        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)   # 保证输出大小等于输入大小\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            # 在官方的实现中，其实是3x3的kernel并不是5x5，这里我也懒得改了，具体可以参考下面的issue\n",
    "            # Please see https://github.com/pytorch/vision/issues/906 for details.\n",
    "            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)   # 保证输出大小等于输入大小\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return torch.cat(outputs, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A classification head for use in RetinaNet.\n",
    "    Args:\n",
    "        in_channels (int): number of channels of the input feature\n",
    "        num_anchors (int): number of anchors to be predicted\n",
    "        num_classes (int): number of classes to be predicted\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_anchors, num_classes, prior_probability=0.01):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "\n",
    "        # class subnet是由四个3x3的卷积层(激活函数为ReLU) + 一个3x3的卷积层(分类器)\n",
    "        conv = []\n",
    "        for _ in range(4):\n",
    "            conv.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n",
    "            conv.append(nn.ReLU(inplace=True))\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "\n",
    "        self.cls_logits = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        all_cls_logits = []\n",
    "\n",
    "        # 遍历每个预测特征层\n",
    "        for features in x:\n",
    "            cls_logits = self.conv(features)\n",
    "            cls_logits = self.cls_logits(cls_logits)\n",
    "\n",
    "            # Permute classification output from (N, A * K, H, W) to (N, HWA, K).\n",
    "            N, _, H, W = cls_logits.shape\n",
    "            cls_logits = cls_logits.view(N, -1, self.num_classes, H, W)\n",
    "            # [N, A, K, H, W] -> [N, H, W, A, K]\n",
    "            cls_logits = cls_logits.permute(0, 3, 4, 1, 2)\n",
    "            # [N, H, W, A, K] -> [N, HWA, K]\n",
    "            cls_logits = cls_logits.reshape(N, -1, self.num_classes)\n",
    "\n",
    "            all_cls_logits.append(cls_logits)\n",
    "\n",
    "        return torch.cat(all_cls_logits, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A regression head for use in RetinaNet.\n",
    "    Args:\n",
    "        in_channels (int): number of channels of the input feature\n",
    "        num_anchors (int): number of anchors to be predicted\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_anchors):\n",
    "        super(RegressionHead, self).__init__()\n",
    "\n",
    "        # box subnet是由四个3x3的卷积层(激活函数为ReLU) + 一个3x3的卷积层(边界框回归器)\n",
    "        conv = []\n",
    "        for _ in range(4):\n",
    "            conv.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n",
    "            conv.append(nn.ReLU(inplace=True))\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "\n",
    "        self.bbox_reg = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.iou_aw = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: List[Tensor]) -> Tensor:\n",
    "        all_bbox_regression = []\n",
    "        all_iou_aware = []\n",
    "\n",
    "        # 遍历每个预测特征层\n",
    "        for features in x:\n",
    "            bbox_regression = self.conv(features)\n",
    "            bbox_regression = self.bbox_reg(bbox_regression)\n",
    "            iou_aware = self.iou_aw(bbox_regression)\n",
    "\n",
    "            # Permute bbox regression output from (N, 4 * A, H, W) to (N, HWA, 4).\n",
    "            N, _, H, W = bbox_regression.shape\n",
    "            # [N, 4 * A, H, W] -> [N, A, 4, H, W]\n",
    "            bbox_regression = bbox_regression.view(N, -1, 4, H, W)\n",
    "            iou_aware = iou_aware.view(N, -1, 4, H, W)\n",
    "            # [N, A, 4, H, W] -> [N, H, W, A, 4]\n",
    "            bbox_regression = bbox_regression.permute(0, 3, 4, 1, 2)\n",
    "            iou_aware = iou_aware.permute(0, 3, 4, 1, 2)\n",
    "            # [N, H, W, A, 4] -> [N, HWA, 4]\n",
    "            bbox_regression = bbox_regression.reshape(N, -1, 4)\n",
    "            iou_aware = iou_aware.reshape(N, -1, 4)\n",
    "\n",
    "            all_bbox_regression.append(bbox_regression)\n",
    "            all_iou_aware.append(iou_aware)\n",
    "\n",
    "        return torch.cat(all_bbox_regression, dim=1), torch.cat(all_iou_aware, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class RetinaNetHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A regression and classification head for use in RetinaNet.\n",
    "    Args:\n",
    "        in_channels (int): number of channels of the input feature\n",
    "        num_anchors (int): number of anchors to be predicted\n",
    "        num_classes (int): number of classes to be predicted\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_anchors, num_classes):\n",
    "        super(RetinaNetHead, self).__init__()\n",
    "        self.classification_head = ClassificationHead(in_channels, num_anchors, num_classes)\n",
    "        self.regression_head = RegressionHead(in_channels, num_anchors)\n",
    "\n",
    "    def forward(self, x: List[Tensor]) -> Dict[str, Tensor]:\n",
    "        return {\n",
    "            \"cls_logits\": self.classification_head(x),\n",
    "            \"bbox_regression\": self.regression_head(x)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "Pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}